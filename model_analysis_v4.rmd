---
title: "model Analysis"
author: "Theodros H."
date: "09/2020"
output: 
  html_document:
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
p {
  font-size: 18px;
}
```

# Abstract




# Introduction

## Objectives

This report describes the four ACT-R models and the learning outcomes produced by the changes in paramters. The report also describes how these models fit behavioral data and details the properties of the best fitting models and parameters. 
The specific objectives of this project is to test if the RLWM task can be modeled well by a group of pure and combined declarative learning models. After fitting the models to participant data we aim to extract parameters that may explain why and how learning resulted as obsereved. If the parameters describe individual differences in learning would the parameters predict other behavioral data like working memory capacity?


## ACT-R Models

Below are the 4 ACT-R models tested. **Note that the bolded names appear through-out this document.** 

- **RL**: Pure RL model based on learning of production utility in ACT-R. learning rate (alpha) and softmax temperature are the only 2 parameters

- **LTM**: A declarative model that solely depends on starage and retirieval of stimuli, response and outcome in ACT-R's declarative memory. This model depends on decay rate, retrieval noise and  

- **meta_RL**: This is a combined RL - LTM model. Information about trials performed by the RL system is shared and stored in LTM (declarative) for use. An isolated (meta) RL system (a set of productions) learns and determines which sub-system, RL or LTM, is used throughout learning. Which subsystem is preferred depends on the specific set of parameters. 

- **biased**: This is a combined RL-LTM model. Information about trials performed by the RL system is not shared with the LTM subsystem. An additional *"strategy"* parameters specifies a **bias** towards the RL model at the 20, 40, 60, and 80 percent of learning and test trials. 

## Approach

Talk about BIC and the model fitting

# Results

```{r set up, echo=FALSE, warning=FALSE, message=FALSE}
rm(list = ls())
library(matlab)
library(reshape2)
library(ggplot2)
library(dplyr)
library(tidyr)
library(MLmetrics)
library(readr)
library(data.table)
library(jsonlite)
library(data.table)
library(knitr)
library(gridExtra)
source('param_influence.R')
library(Rmisc)
library(ggpubr)

knitr::opts_chunk$set(
  comment = "#>", echo = FALSE, warning = FALSE, 
  message = FALSE
)


```

```{r import data,  message=FALSE, warning=FALSE}
#----- import subject data
# 
subjects = read.csv("./RLWM_data/wmo_subjects_across_studies_031820.csv", header = F)
colnames(subjects)='subjects'
sdat = fread('./RLWM_data/all_subject_n83_learn_test_data.csv', header = T) %>% t()
# sdat contains data fro 83 participants (columns), 
# rows 1:12 learn accuracy set 3 ; 
# rows 13:24 learn accuracy set 6 ;
# row 25 test set 3 accuracy ;
# row 26 test set 6 accuracy ;

#------ Modify subject data to 'weight' accuracy in test for 3 and 6 by repeating each 12x 
sdat.temp <- rep(sdat[ , 25:26], 12) %>% 
  as.matrix() %>% 
  reshape(., 996,2) %>% 
  reshape(., 166,12) 

sdat.mod  <- cbind(sdat[, 1:24], 
                   sdat.temp[1:83,], 
                   sdat.temp[84:166, ]) 




 
 #--------- Integrated model RL to LTM pipe
simsRL_LTMpipe <- fromJSON('./simulated_data/pipe_model/pipe_all_data_5params_071320.JSON')

 simsRL_LTMpipe.set3learn <- simsRL_LTMpipe$data$set3_learn %>%
  unlist() %>%
  as.matrix() %>%
  reshape(., 12, nrow(simsRL_LTMpipe$data)) %>%
  t()

simsRL_LTMpipe.set6learn <- simsRL_LTMpipe$data$set6_learn %>% 
  unlist() %>% 
  as.matrix() %>%
  reshape(., 12, nrow(simsRL_LTMpipe$data)) %>%
  t() 

simsRL_LTMpipe.s3s6test.temp <-
  simsRL_LTMpipe$data$set3_test %>%
  cbind(., simsRL_LTMpipe$data$set6_test) %>% 
   rep(., 12) %>%
   as.matrix() %>%
   reshape(., nrow(simsRL_LTMpipe$data) * 12, 2) %>%
   reshape(., nrow(simsRL_LTMpipe$data) *2, 12) 

 RL_LTMpipe.sim.dat <- cbind(simsRL_LTMpipe.set3learn, 
                         simsRL_LTMpipe.set6learn, 
                         simsRL_LTMpipe.s3s6test.temp[1:nrow(simsRL_LTMpipe$data), ], 
                         simsRL_LTMpipe.s3s6test.temp[3126 :  nrow(simsRL_LTMpipe.s3s6test.temp), ], #copy the seond half of columns out
                         simsRL_LTMpipe$data[,c('bll','alpha','egs','imag','ans','strtg')]) %>% 
   data.table()

 
 
 #--------- Integrated model assigned strategy
simsRL_LTMstr <- fromJSON('./simulated_data/strategy_model/strategy_all_data_5params_071320.JSON')

 simsRL_LTMstr.set3learn <- simsRL_LTMstr$data$set3_learn %>%
  unlist() %>%
  as.matrix() %>%
  reshape(., 12, nrow(simsRL_LTMstr$data)) %>%
  t()

simsRL_LTMstr.set6learn <- simsRL_LTMstr$data$set6_learn %>% 
  unlist() %>% 
  as.matrix() %>%
  reshape(., 12, nrow(simsRL_LTMstr$data)) %>%
  t() 

simsRL_LTMstr.s3s6test.temp <-
  simsRL_LTMstr$data$set3_test %>%
  cbind(., simsRL_LTMstr$data$set6_test) %>% 
   rep(., 12) %>%
   as.matrix() %>%
   reshape(., nrow(simsRL_LTMstr$data) * 12, 2) %>%
   reshape(., nrow(simsRL_LTMstr$data) *2, 12) 

 RL_LTMstr.sim.dat <- cbind(simsRL_LTMstr.set3learn, 
                         simsRL_LTMstr.set6learn, 
                         simsRL_LTMstr.s3s6test.temp[1:nrow(simsRL_LTMstr$data), ], 
                         simsRL_LTMstr.s3s6test.temp[(nrow(simsRL_LTMstr$data)+1) : nrow(simsRL_LTMstr.s3s6test.temp), ],
                         simsRL_LTMstr$data[,c('bll','alpha','egs','imag','ans', 'strtg')]) %>% 
   data.table()
 
 

#--------- Reinforcement Learning Model

simsRL <- fromJSON('./simulated_data/RL_model/RL_all_data_5params_071329.JSON')

 simsRL.set3learn <- simsRL$data$set3_learn %>%
  unlist() %>%
  as.matrix() %>%
  reshape(., 12, nrow(simsRL$data)) %>%
  t() 

 
 
simsRL.set6learn <- simsRL$data$set6_learn %>% 
  unlist() %>% 
  as.matrix() %>%
  reshape(., 12, nrow(simsRL$data)) %>%
  t() 

 simsRL.s3s6test.temp <-
  simsRL$data$set3_test %>%
  cbind(., simsRL$data$set6_test) %>% 
   rep(., 12) %>%
   as.matrix() %>%
   reshape(., nrow(simsRL$data)*12, 2) %>%
    reshape(., nrow(simsRL$data)*2, 12) 
 

 RL.sim.dat <- cbind(simsRL.set3learn, 
                     simsRL.set6learn, 
                     simsRL.s3s6test.temp[1:nrow(simsRL$data), ], 
                     simsRL.s3s6test.temp[(nrow(simsRL$data)+1):nrow(simsRL.s3s6test.temp), ] , 
                     simsRL$data[,c('bll','alpha','egs','imag','ans')]) %>%  
   data.table()

#--------- Longterm Memory/WM/Declarative model 
 
simsLTM    <- fromJSON('./simulated_data/LTM_model/LTM_all_data_5params_071320.JSON')

 simsLTM.set3learn <- simsLTM$data$set3_learn %>%
  unlist() %>%
  as.matrix() %>% 
  reshape(., 12, nrow(simsLTM$data)) %>% 
  t()

simsLTM.set6learn <- simsLTM$data$set6_learn %>% 
  unlist() %>% 
  as.matrix() %>%
  reshape(., 12, nrow(simsLTM$data)) %>%
  t()

simsLTM.s3s6test.temp <-  simsLTM$data$set3_test %>%
  cbind(., simsLTM$data$set6_test) %>% 
   rep(., 12) %>% 
   as.matrix() %>% 
   reshape(., 1500, 2) %>% 
   reshape(., nrow(simsLTM$data)*2, 12) #54= nrow(sims.LTM) * 2

 
LTM.sim.dat <- cbind(simsLTM.set3learn, 
                      simsLTM.set6learn, 
                      simsLTM.s3s6test.temp[1:nrow(simsLTM$data), ], 
                      simsLTM.s3s6test.temp[(nrow(simsLTM$data)+1):nrow(simsLTM.s3s6test.temp), ],
                     simsLTM$data[,c('bll','alpha','egs','imag','ans')])  %>% 
   data.table()


 
 # LTM.sim.dat <- cbind('set3_learn'=simsLTM.set3learn %>% c(), 
  #                    'set6_learn'=simsLTM.set6learn %>% c(),
   #                   'time'=rep(1:12,324/12),
    #                  'set3_test'=simsLTM.s3s6test.temp[1:27, ] %>% c(), 
     #                 'set6_test'=simsLTM.s3s6test.temp[28:54, ] %>% c(), 
      #                simsLTM$data[,c('bll','alpha','egs','imag','ans')]) %>% 
   #data.table()
 
 tmp.color = c('#ca0020','#f4a582' ,'#0571b0','#92c5de')
 
```

```{r fit model data to subject data, message=FALSE, warning=FALSE}

#(1) Transform RMSE into residual sum of squares by doing RSS = RMSE^2 * n
#11:34
#(2) Calculate BIC as: BIC = n + n log (2*pi) + n log (RSS/n) + log(n) * (k + 1)
#11:36
#In RL, k = 2; in LTM, k = 3; and Integrated, k = 5 or k = 6




Andys_BIC <- function(rmse, k) {
  
  # RSS first
  n = 48 #lean3 + learn 6 + (test3)*12 + (test6)*12
  RSS <- ((rmse)^2) * n
  
  # BIC next
  bic <- n + (n * log(2*pi)) + (n * log(RSS/n)) + (log(n) * (k + 1))
  
  return(bic)
}


#----- loop through subject data and check for fit against model data using mean squared error. 


mseRL.temp         =c()
mseLTM.temp        =c()
mseRL_LTMorig.temp =c()
mseRL_LTMpipe.temp =c()
mseRL_LTMstr.temp  =c()

for(s in c(1:nrow(sdat.mod))) { # for each subject
 # model 1 
 mseRL.temp     <- rbind(mseRL.temp, apply(RL.sim.dat[, 1:48], 1, function(x,y) MSE(x, sdat.mod[s, ])) %>% sqrt()
                         )
 # model 2
 mseLTM.temp    <- rbind(mseLTM.temp, apply(LTM.sim.dat[, 1:48], 1, function(x,y) MSE(x, sdat.mod[s, ] )) %>% sqrt()
                         )
 
 # model 3.1
 mseRL_LTMpipe.temp <- rbind(mseRL_LTMpipe.temp, 
                             apply(RL_LTMpipe.sim.dat[, 1:48], 1, function(x,y) MSE(x, sdat.mod[s, ])) %>% sqrt()
                             )
 # model 3.2
mseRL_LTMstr.temp  <- rbind(mseRL_LTMstr.temp, 
                            apply(RL_LTMstr.sim.dat[, 1:48], 1, function(x,y) MSE(x, sdat.mod[s, ])) %>% sqrt()
                            )
  
}

#------ Exrtact row indices to get parameter set of parameters for best fit model

#------------first find the smallest BIC
RL.bic = Andys_BIC(mseRL.temp, 2)
LTM.bic = Andys_BIC(mseLTM.temp, 3)
RL_LTMpipe.bic = Andys_BIC(mseRL_LTMpipe.temp, 5)
RL_LTMstr.bic = Andys_BIC(mseRL_LTMstr.temp, 6)

RL.fit     = as.matrix(apply(RL.bic , 1, min))   
LTM.fit    = as.matrix(apply(LTM.bic , 1, min))  

RL_LTMpipe.fit = as.matrix(apply(RL_LTMpipe.bic, 1, min)) 
RL_LTMstr.fit = as.matrix(apply(RL_LTMstr.bic , 1, min))



#-------------second, find actual row number using smallest value


ind.temp.RL <- c()

ind.temp.RL_LTMstr <- c()
ind.temp.RL_LTMpipe <- c()
ind.temp.LTM <- c()

for ( i in 1:length(RL.fit)) {
  ind.temp.RL <- rbind(ind.temp.RL, which(RL.bic[i,] %in% RL.fit[i]))
  
}

for ( i in 1:length(LTM.fit)) {
  ind.temp.LTM <- rbind(ind.temp.LTM, which(LTM.bic[i,] %in% LTM.fit[i]))
  
}

for ( i in 1:length(RL_LTMpipe.fit)) {
  ind.temp.RL_LTMpipe <- rbind(ind.temp.RL_LTMpipe, 
                           which(RL_LTMpipe.bic[i,] %in% RL_LTMpipe.fit[i]))
  
}

for ( i in 1:length(RL_LTMstr.fit)) {
  ind.temp.RL_LTMstr <- rbind(ind.temp.RL_LTMstr, which(RL_LTMstr.bic[i,] %in% RL_LTMstr.fit[i]))
  
}


#--------which model fits a participant most?
#1= RL; 2= LTM; 3 = RL_LTMpipe; 4 = RL_LTMstr


#--------------There are no min.col functions? Work around find the max after inverting:
participants.fit = c()
model.fits <- data.frame(RL.fit, LTM.fit, RL_LTMpipe.fit, RL_LTMstr.fit)#, fit.labels)
participant.min <- apply(model.fits, 1, min)
     
for (s in 1:nrow(model.fits)){

       participants.fit = rbind(participants.fit, which(participant.min[s] == model.fits[s,]))
     }
          
```

```{r attach model and behavioral data }

#1= RL; 2= LTM; 3 = RL_LTMpipe; 4 = RL_LTMstr

RL.p.model   <- data.frame('subjects' = subjects[participants.fit==1],
                           'model' = rep('RL', sum(participants.fit==1)),
                           's3'= RL.sim.dat[ind.temp.RL[participants.fit==1 ],1:12],
                           's6'= RL.sim.dat[ind.temp.RL[participants.fit==1 ],13:24],
                           'bll' = rep(NA, sum(participants.fit==1)) %>% as.double(),
                           'alpha' = RL.sim.dat[ind.temp.RL[participants.fit==1],50],
                           'egs' = RL.sim.dat[ind.temp.RL[participants.fit==1],51],
                           'imag' = rep(NA, sum(participants.fit==1)) %>% as.double(),
                           'ans' = rep(NA, sum(participants.fit==1)) %>% as.double(),
                           'bias' = rep(NA, sum(participants.fit==1)) %>% as.double(),
                           "s3test"=RL.sim.dat[ind.temp.RL[participants.fit==1 ],25],
                           's6test' = RL.sim.dat[ind.temp.RL[participants.fit==1 ],37]
                           ) 

                          
LTM.p.model  <- data.frame( 'subjects' = subjects[participants.fit==2],
                            'model' = rep('LTM', sum(participants.fit==2)),
                            's3'= LTM.sim.dat[ind.temp.LTM[participants.fit==2],1:12],
                           's6'= LTM.sim.dat[ind.temp.LTM[participants.fit==2],13:24],
                           'bll' = LTM.sim.dat[ind.temp.LTM[participants.fit==2],49],
                           'alpha' = rep(NA, sum(participants.fit==2)) %>% as.double(),
                           'egs' = rep(NA, sum(participants.fit==2)) %>% as.double(),
                           'imag' = LTM.sim.dat[ind.temp.LTM[participants.fit==2],52],
                           'ans' = LTM.sim.dat[ind.temp.LTM[participants.fit==2],53],
                           'bias'= rep(NA, sum(participants.fit==2)) %>% as.double(),
                           's3test' = LTM.sim.dat[ind.temp.LTM[participants.fit==2],25] %>% data.frame(),
                           's6test' = LTM.sim.dat[ind.temp.LTM[participants.fit==2],37], fix.empty.names=T)

pipe.p.model <- data.frame( 'subjects' = subjects[participants.fit==3],
                            'model' = rep('meta_RL', sum(participants.fit==3)),
                           's3'= RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],1:12],
                           's6'= RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],13:24],
                            'bll' = RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],49],
                           'alpha' = RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],50],
                           'egs' = RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],51],
                           'imag' = RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],52],
                           'ans' = RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],53],
                           'bias' =  RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],54] %>% 
                             unlist() %>% 
                             as.numeric(),
                            's3test' = RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],25],
                           's6test' = RL_LTMpipe.sim.dat[ind.temp.RL_LTMpipe[participants.fit==3],37]
                           )

str.p.model  <- data.frame( 'subjects' = subjects[participants.fit==4],
                            'model' = rep('biased', sum(participants.fit==4)),
                           's3'= RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],1:12],
                           's6'= RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],13:24],
                            'bll' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],49],
                           'alpha' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],50],
                           'egs' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],51],
                           'imag' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],52],
                           'ans' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],53],
                           'bias' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],54] %>%
                             separate(col='strtg',sep="(?<=[A-Z])(?=[0-9])", into = c( NA,'bias'), convert = TRUE) %>% 
                             c() %>%
                             unlist() %>% 
                             as.double()/100,
                            's3test' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],25],
                           's6test' = RL_LTMstr.sim.dat[ind.temp.RL_LTMstr[participants.fit==4],37]
                 ) 


join.model.dat <- rbind(RL.p.model, LTM.p.model, pipe.p.model, str.p.model )

colnames(sdat) <- c(colnames(RL.p.model[3:26]),'s3.13','s6.13')
colnames(join.model.dat)[33:34]=c('s3.13','s6.13')

p.behav.model  = merge(cbind(subjects, sdat), 
                 join.model.dat, by = c("subjects"), 
                 suffixes = c('.behav', '.model'), sort=FALSE)
 

melted.p.behav.model <- p.behav.model %>% 
  reshape2::melt(id.vars = c("subjects", "model" ,'bll','alpha','egs','imag','ans', 'bias' ), value.name="accuracy", variable.name="condition") %>% 
     separate("condition", into = c('setSize', "iteration","type"), remove = FALSE, convert = TRUE) %>% 
  unite("cond.model", c(setSize,type), remove = FALSE)

plot.indiv <- function(this.model, title, columns) {
 
  melted.p.behav.model %>% 
    filter(iteration != 13) %>% 
    filter(model == this.model) %>% 
    ggplot(aes(as.numeric(iteration), accuracy, color=cond.model, group=cond.model)) + 
    geom_point() +
    geom_line(size=1) +
    facet_wrap(vars(subjects), ncol = columns, scales = 'free')+
    scale_color_brewer(palette = "Paired")+
    theme_pubr(base_size = 16) +
    ggtitle(title)
  
}



```

 The LTM model fits the most number of participants (`r sum(participants.fit==2)`) followed by the biased version of the combined RL-LTM model (`r sum(participants.fit==4)`) and the meta-RL combined model at third (`r sum(participants.fit==3)`). The RL only model has only one participant that fit it best. 

```{r model fit plots:group fit bar plot, fig.cap = "Figure 1."}
#fit plots
#participants.fit %>% 
 # hist(main='Counts of participants by model', xlab = ("1= RL; 2= LTM; 3=RL_LTM"), #lwd=4.3)

fit.labels <- ifelse(participants.fit==2, 'LTM', 
                     ifelse(participants.fit==3, 'meta_RL',
                            ifelse(participants.fit==4,'biased','RL')
                            )
                     )

models.name=c('RL', 'LTM', 'meta_RL', 'biased')
data.frame('model'= participants.fit)  %>% 
 
   ggplot(aes(factor(model), fill=factor(model))) + 
  geom_bar() +
   ggtitle('Counts of participants by model') +
 # theme_minimal(base_size = 20)+
  scale_x_discrete(labels=models.name )+
  xlab("Models")+
   scale_fill_brewer( palette = "Paired")+
theme(legend.position='none') +
  theme_pubr(base_size = 16, legend = 'none') 
  

```

```{r unique fitting model counts}
nRL = ind.temp.RL[participants.fit==1 ] %>% unique() 
nLTM  = ind.temp.LTM[participants.fit==2 ] %>% unique()
nBias = ind.temp.RL_LTMstr[participants.fit==4 ] %>% unique()
nMeta = ind.temp.RL_LTMpipe[participants.fit==3 ] %>% unique()


```
 
 There is only `r length(nRL)` RL best fitting model. For the most popular model, LTM, that fit  (`r sum(participants.fit==2)`) participants, there are only `r length(nLTM)` best fitting parameter sets. The biased model seems to be the most diverse at `r length(nBias)` parameter sets for  (`r sum(participants.fit==4)`) participants. The meta-RL model closely follows the biased model interms of diversity of parameter sets at `r length(nMeta)` parameter sets for  (`r sum(participants.fit==3)`) subjects. 
 
### BIC value descriptions 

The following two boxplots (figures 2 and 3) show the medians and ranges of the BIC values that determined that the LTM model is the best fitting model. Boxplot 1 shows the range in BIC across all participants whether they fit that model best or not and therefore has equal number of data points (83). The second boxplot however displays BIC medians and ranges for only the best fitting participants for that data (that is why the plot for RL is a line representing the only data point). 

```{r  BOX PLOT 1, fig.cap = "Figure 2."}
model.fits %>% 
  reshape2::melt() %>% 
  ggplot(aes(y=value,variable, group=variable)) +
  geom_boxplot() + 
  ggtitle('BIC: All participants')+
  xlab('model')+
  scale_x_discrete(labels=models.name )+
  #theme_bw() +
  theme(legend.position='none') +
   scale_colour_brewer( palette = "Set1") +
 theme_pubr(base_size = 16) 
```

This second boxplot was in an effort determine how well a model type fits its preferred set of behavioral data. This might be redundant. The LTM model fits data much better than the other group (This might need a statistical test). 

```{r model fit plots: group fit BOXPLOT2, fig.cap = "Figure 3."}

selector <- c(participants.fit==1,participants.fit==2,participants.fit==3,participants.fit==4)

model.fits %>% 
  reshape2::melt() %>% 
  cbind(selector) %>% 
  filter(selector==TRUE) %>% 
  ggplot(aes(y=value,variable, group=variable)) +
  geom_boxplot() + 
  ggtitle('BIC: best fitting per model')+
  xlab('model')+
  theme_bw() +
  theme(legend.position='none') +
   scale_colour_brewer( palette = "Set1") +
  theme_pubr(base_size = 16) 
```

### Assesments of Model fits 

Looking at the learning curves for the four models in Figure 4, the differences in learning rates are apparent as are other features like the separation between  the two set sizes. In the plot below each data point is the average accuracy, for that number of stimulus presentations, across all parameter combinations. The LTM and RL models predict that an increase in set-size does not diminish learning rate and accuracy. But this analysis washes out the individual differences that could be captured by the diverse set of parameter combinations.  

```{r models only, fig.cap = "Figure 4."}
cbind('RL.s3'=RL.sim.dat[,1:12], 'RL.s6'=RL.sim.dat[,13:24],
      'LTM.s3'=LTM.sim.dat[, 1:12], 'LTM.s6'=LTM.sim.dat[, 13:24],
      'metaRL.s3'=RL_LTMpipe.sim.dat[,1:12], 'metaRL.s6'=RL_LTMpipe.sim.dat[,13:24], 
      'bias.s3'=RL_LTMstr.sim.dat[,1:12],  'bias.s6'=RL_LTMstr.sim.dat[,13:24]) %>% 
  reshape2::melt(value.name="accuracy", variable.name="condition") %>% 
  separate("condition", into = c('model', 'setSize','iteration'), convert = T) %>% 
  summarySE(measurevar = 'accuracy', groupvars = c('model','setSize','iteration')) %>% 
  ggplot(aes(iteration,accuracy, color=setSize)) +
  geom_point() +
  geom_line(size=1) +
  facet_wrap(vars(model)) +
  scale_color_brewer(palette = "Paired") +
  theme_pubr(base_size = 16) 

```
 
The panels in figure 5 show the mean accuracy for particant behavioral data. The model lines are averages across parameters for that group only. 
 As we are aiming for an individual differences look at these data, collapsing across so much of this variablility is uninformative, as was shown above in figure 4,especially if the differences, once fit to actual behavioral data, indicate large differences in learning outcomes or cogntive faculty diagnostics like working memory capacity. Here, only the best fitting sets of parameter combinations were selected and collapsed. As can be seen in the figure below, the different model types appear to be vastly different and some charateristics of behavioral data have come through, such as the separations of the learning trajectories for the different setsizes in the RL-LTM Biased model fit. It can also be seen that some paramter sets in the LTM model also capture the diffculty associated with increasing set size (solid lines in Fig. 5B). T The LTM participants, on average have the highest accuracies for the testing phase in both set sizes but they are nearly indistinguishable from the meta-RL group for accuracy at end of learning. The biased group shows the most separation between the set size 3 and 6 at learningand also lower accuracy at test than LTM. The biased group  is negligibly different from the meta-RL group for set size 3 but shows a marked difference at set size 6, closely following the behavioral data. 
 
```{r, fig.width=12, fig.height=12, fig.cap="Figure 5." }
 melted.p.behav.model %>% 
   filter(iteration != 13) %>% 
  summarySE( measurevar = "accuracy", groupvars = c("iteration", "condition", "cond.model", "model")) %>% 
  ggplot(aes( as.factor(iteration), accuracy, color=cond.model, group=cond.model)) +
  geom_point() +
  geom_line(size=1) +
  geom_errorbar(aes(ymin=accuracy-se, ymax=accuracy+se), width=.35, size=1) +
  facet_wrap(vars(model)) +
  scale_color_brewer(palette = "Paired") +
  xlab('stimulus iteration') +
 theme_pubr(base_size = 16)  
  


```

There are five outcome measures of interest in the RLWM task: accuracy at the end learning, accuracy at test, learning rate characterized as number of stimulus presentations to reach 95% accuracy, the differences in learning of set3 and set 6 and also the level of preserved learning at test for both set-sizes. The following analysis compares the model data with behavioral data. 

```{r stats for differneces in RLWM task for the 4 groups}
#---- measures for RLWM:  T THESE SHOULD BE EDITED TO SHOW BOTH MODEL AND BEHAVIORAL DATA

 #------Learning accuracy (12th iteration, done)
 #  melted.p.behav.model %>% 
 #  filter(iteration==12) %>% #Grab only the 12th iteration
 #  summarySE( measurevar = "accuracy", groupvars = c( "model","cond.model")) %>% 
 #   ggplot(aes(x=model,weight=accuracy,ymin=accuracy-se, ymax=accuracy+se, group=cond.model,  color=cond.model)) +
 #  #geom_jitter(height = 0, width = 0.1) +
 # #geom_bar(position =position_dodge(), aes(y=accuracy), stat = 'identity')+
 #  geom_errorbar(position=position_dodge(width = 0.75), width=.55, size=2) +
 #  geom_point(position =position_dodge(width = 0.75), aes(y=accuracy, color=cond.model), size=4) +
 #   scale_color_brewer(palette = "Paired") +
 #  ylim(c(0.25,1))+
 # # xlab('stimulus iteration') +
 # theme_pubr(base_size = 24)  
 #  # facet_wrap(vars(model)) 

 #------Test accuracy
# melted.p.behav.model %>% 
#    filter(iteration == c(13)) %>% 
#   summarySE( measurevar = "accuracy", groupvars = c("iteration", "condition", "cond.model", "model")) %>%# View()
#   ggplot(aes(x=model,weight=accuracy,ymin=accuracy-se, ymax=accuracy+se, group=cond.model,  color=cond.model)) +
#   #geom_jitter(height = 0, width = 0.1) +
#  #geom_bar(position =position_dodge(), aes(y=accuracy), stat = 'identity')+
#   geom_errorbar(position=position_dodge(width = 0.75), width=.55, size=2) +
#   geom_point(position =position_dodge(width = 0.75), aes(y=accuracy, color=cond.model), size=4) +
#    scale_color_brewer(palette = "Paired") +
#   ylim(c(0.25,1))+
#  # xlab('stimulus iteration') +
#  theme_pubr(base_size = 24, legend = 'none')  
#   # facet_wrap(vars(model)) 
```

```{r Learning and test combined plot, fig.cap="figure 6"}
#------Learning and test combined
melted.p.behav.model %>% 
  filter(iteration>=12) %>% #Grab both the 12th iteration and test
  summarySE( measurevar = "accuracy", groupvars = c( "model","cond.model", "iteration")) %>% 
   ggplot(aes(x=as.factor(iteration),weight=accuracy,ymin=accuracy-se, ymax=accuracy+se, group=cond.model,  color=cond.model)) +
  geom_errorbar(position=position_dodge(width = 0.1), width=.55, size=1) +
  geom_point(position =position_dodge(width = 0.1), aes(y=accuracy, color=cond.model), size=1) +
   geom_line(position =position_dodge(width = 0.1), aes(y=accuracy, color=cond.model), size=1) +
   scale_color_brewer(palette = "Paired") +
  ylim(c(0.25,1))+
  xlab('condition') +
 theme_pubr(base_size = 18)  +
  scale_x_discrete(labels=c("learn","test"))+
   facet_wrap(vars(model))  
```


```{r Learning rate, fig.cap="Figure 7."}
 #------Learning rate(number of trials to 90%)

matcher <- function(ths){ #wrap up match function to make it easier to use with apply function
  return(
    match(TRUE, ths)
    )
  }
acc.level = .84

learning.Rate <- 
  cbind(subjects,
       "model"= p.behav.model$model, 
        's3_behav'= apply(sdat[,1:12]>acc.level, 1, matcher) %>% 
          as.numeric(),
        's6_behav'= apply(sdat[,13:24]>acc.level, 1, matcher) %>% 
          as.numeric(),
        's3_model'= apply(join.model.dat[,3:14]>acc.level, 1, matcher) %>% 
          as.numeric(),
        's6_model'= apply(join.model.dat[,15:26]>acc.level, 1, matcher) %>% 
          as.numeric()
        ) %>% as.data.frame() 
  


  
  learning.Rate %>%
    reshape2::melt(id.vars = c("subjects","model"), value.name="learning.rate") %>% 
      separate("variable", into = c('setSize','type'), remove = FALSE, convert = TRUE) %>% 
    dplyr::group_by(setSize, model, type) %>% 
    dplyr::summarise('mean'=mean(learning.rate,na.rm=T), 
                     'SD'= sd(learning.rate, na.rm = T),
                     'se'= sd(learning.rate, na.rm = T) / sqrt(sum(!is.na(learning.rate))),
                     'n'= sum(!is.na(learning.rate))
                     ) %>% 
    ggplot(aes(mean,x=setSize, group=type, color=type)) +
    #geom_histogram(stat = 'count') +
  geom_point(size=2)+
    geom_errorbar(aes(ymax=mean+se,ymin=mean-se), width=.25, size=1.5)+
      scale_color_brewer(palette = "Paired") +
   theme_pubr()+
    facet_wrap(vars(model))+
    ggtitle('rate: n trials to 85%')
  
#----- Alternative learning rate
 fit.lr <-  melted.p.behav.model %>% 
    filter(iteration<7 & iteration!=13) %>% # get the first 6 learning trials to model
    dplyr::group_by(subjects,setSize, type,model) %>% 
    do( broom::tidy(lm(accuracy~iteration, data=.))[2] %>% 
          as_data_frame()) %>% 
   cbind('estimate.type'= rep(c('y-int','slope'), 83*2*2)) 
 
 fit.lr %>% 
   filter(estimate.type=='slope') %>% 
   group_by(setSize,type, model) %>%
   dplyr::summarize('mean'= mean(estimate),
                    'se' = sd(estimate, na.rm = T) / sqrt(sum(!is.na(estimate)))) %>% 
   ggplot(aes(mean,x=setSize, group=type, color=type)) +
    #geom_histogram(stat = 'count') +
  geom_point(size=2)+
    geom_errorbar(aes(ymax=mean+se,ymin=mean-se), width=.25, size=1.5)+
      scale_color_brewer(palette = "Paired") +
   theme_pubr()+
    facet_wrap(vars(model)) +
   ggtitle("rate: beta estimate of first 6 iterations")
   
 
 
 
  
```

```{r Analysis of separation between the curves, fig.cap="Figure 8."}
   #------Separation between the curves (done)
  
#  s3s6Diff <- 
    melted.p.behav.model %>% 
     filter(iteration!=13, type=='behav') %>% # get everything but test
    select(c('subjects','model', 'accuracy','iteration', 'setSize')) %>% 
   reshape2::dcast(setSize ~ iteration + subjects + model , value.var = 'accuracy') %>% #change in to wide form to subrtract b/n s3 and s6
   select(-setSize) %>% #not needed
    as.matrix() %>% #required to use diff() function
  diff.default() %>%  #take the difference b/n the set sizes
  #  abs() %>% #take the absolute value (I guess it doesn't matter if s3 is higher than s6 or viceversa)
    reshape2::melt() %>% 
    select(-Var1) %>% 
      separate("Var2", into = c('iteration', "subjects","model"), remove = T, convert = TRUE) %>% 
    summarySE(measurevar = "value", groupvars = c("subjects", "model")) %>% 
    select(subjects, model,"learnDiff"= value)
  
    
``` 

```{rChange from learning to test, fig.cap="Figure 9."}

   
    #---------Change from learning to test
   learnTestDiff <- 
     melted.p.behav.model %>% 
       filter( iteration>= 12, type=='behav') %>% 
        select(c('subjects','model', 'accuracy','iteration', 'setSize')) %>% 
       reshape2::dcast(iteration ~ setSize + subjects + model, value.var = 'accuracy') %>% 
      select(-iteration) %>% #not needed
    as.matrix() %>% #required to use diff() function
  diff.default() %>%  #take the difference b/n the learn and test
  #  abs() %>% #take the absolute value (I guess it doesn't matter if s3 is higher than s6 or viceversa)
    reshape2::melt() %>% 
       select(-Var1) %>% 
      separate("Var2", into = c('setSize', "subjects","model"), remove = T, convert = TRUE)
 
    learnTestDiff  %>% 
    summarySE(measurevar = "value", groupvars = c( "model","setSize"))%>% 
  ggplot(aes(model, value, color=setSize)) + 
      geom_point() + 
      geom_errorbar(aes(ymax=value+se, ymin=value-se)) +
      theme_pubr() +
      ggtitle("Change from learning to test")+
      ylim(c(-0.6,0))
      
 
  

```

It is difficult to assess what the model fits are capturing without examining the specific paramter sets more carefully or deducing if membership in a particular model group predicts some other cognitve or learning aspects of the subjects. 
First, for the cohort of subjects

```{r test data plots}


```

### What are the differences in learning type interms of behavioral outcomes in other tasks?


These plots show group effects for uCLIMB subjects only in python and OLCTS measures and behavioral predictors. 
```{r import uCLIMB data}

uclimb <- read.csv("RLWM_data/python_olcts_uclimb_summary_noEEG_092320.csv", header = T)
uclimb.model.temp1 <- uclimb %>%  merge(p.behav.model, by = "subjects") 
uclimb.model.dat <-  cbind(uclimb.model.temp1, 'wmComposite'= uclimb.model.temp1[,c("RspanScore", "SspanAbsoluteScore","OspanAbsoluteScore") ] %>% rowMeans())


#specific goals are to compare to:
#1 learning rate in language 
#2 Learning rate in Python
#3 Post-test Python
#4 Post-test Language
#5 WM composite score
#6 RAvens
#7 PSS measures

uclimb.model.dat %>% 
  select("MLAT_Total", "Ravens.Score","NDV", "wmComposite","RspanTotal", "PSS_ChooseA.ACC" , "PSS_AvoidB.ACC", "PSS_All.ACC" ) %>%
  scale(center = F)  %>% 
  data.frame('model'=uclimb.model.dat$model) %>% 
  reshape2::melt() %>% 
  summarySE(groupvars = c("model","variable" ), measurevar = "value") %>% 
  ggplot(aes(model, value, group=model,fill=model))+
  geom_bar(stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin=value-se, ymax=value+se), width=.25, size=.75)+
 theme_pubr(base_size = 16)+
  facet_wrap(vars(variable))
  
  # Python only
uclimb.python <- read.csv("RLWM_data/python_uclimb_summary_noEEG_092320.csv", header = T)
uclimb.python.temp1 <-  uclimb.python %>%  merge(p.behav.model, by = "subjects") 

python.LR <- read.delim("RLWM_data/SlopeCoefficients_UpTo10_python20181222.txt", sep="\t", header = T)

uclimb.python.dat <- uclimb.python.temp1 %>% 
  merge(python.LR, by="subjects")    


uclimb.python.dat %>% 
  select( "PostTest_Total_ACC","python.learning.coeff","PT_Syntax_ACC" ,"PT_Semantics_ACC","RPS.Code") %>%
  scale(center = F)  %>% 
  data.frame('model'=uclimb.python.dat$model) %>% 
  reshape2::melt() %>% 
  summarySE(groupvars = c("model","variable" ), measurevar = "value") %>% 
  ggplot(aes(model, value, group=model,fill=model))+
  geom_bar(stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin=value-se, ymax=value+se), width=.25, size=.75)+
 theme_pubr(base_size = 16)+
  facet_wrap(vars(variable))


#combined Python and OLCTS measures
#learning rate
# "Slope_OLCTS" and python learning rate
# syntax 
# "PT_Syntax_ACC_Python" and "GR_Total_OLCTS"
# vocab
#PT_Vocab_OLCTS and "PT_Semantics_ACC_Python" 

```

We have 3Back and PSS for a large majority of participants -  what are the group differences if any in these outcomes based on model fit?

```{r paramter and group effects on 3Back and PSS}
pss3back.uClmb.STG <- read.csv('RLWM_data/STAG_uCLIMB_PSS_Nback_data_100120.csv', header = T)

# merge with modeling data
pss3Back.mergr <- 
  p.behav.model %>%
  select('subjects', 'model','alpha','egs','bll','imag','ans') %>% 
  merge(pss3back.uClmb.STG, by='subjects')
#,'alpha','egs','bll','imag','ans'

#melt and plot
pss3Back.mergr %>% 
   select(-subjects, -model) %>% 
  scale(center = F) %>% 
  data.frame('model'=pss3Back.mergr$model) %>% 
  reshape2::melt() %>% 
  summarySE(measurevar = 'value', groupvars = c('model','variable')) %>% 
  ggplot(aes(model, value, group=model,fill=model))+
  geom_bar(stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin=value-se, ymax=value+se), width=.25, size=.75)+
 theme_pubr(base_size = 16)+
  facet_wrap(vars(variable))
 
```

```{r group effects on span tasks}
realm_span <- read.csv('RLWM_data/REALM_uCLIMB_WM_Span_Task_Results_Fall_2019.csv', header = T)

#compute composite WM score
realm_span.temp1 <-
 realm_span %>% 
    select(-'subjects') %>% 
    scale(center = F) %>% 
    rowMeans(na.rm=T) %>% cbind(realm_span, 'compositeWM'=.)
  
realm_span.mergr <- 
   p.behav.model %>%
  select('subjects', 'model') %>% 
  merge(realm_span.temp1, by='subjects') 
  #,'alpha','egs','bll','imag','ans'

#melt and plot

 realm_span.mergr %>% 
   select(-subjects, -model, -compositeWM) %>% 
  scale(center = F) %>% 
  data.frame('model'=realm_span.mergr$model, 
             'compositeWM'=realm_span.mergr$compositeWM) %>%
  reshape2::melt() %>%  
  summarySE(measurevar = 'value', groupvars = c('model','variable'), na.rm = T) %>% 
  ggplot(aes(model, value, group=model,fill=model))+
  geom_bar(stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin=value-se, ymax=value+se), width=.25, size=.75)+
 theme_pubr(base_size = 16)+
  facet_wrap(vars(variable))

```

Chantel's request: combine language and programming measures and compare groups.

```{r combined OLCTS and PYTHON measures}

combined_measures = data.frame(
                               "model"= uclimb.model.dat[,"model"],
                               "learning_rate"= uclimb.model.dat[,c( "Slope_OLCTS","LearningRate_Python")] %>%
                                 scale(center = F) %>%
                                 rowMeans(na.rm=T),
                               "semantics"= uclimb.model.dat[,c( "PT_Vocab_OLCTS", "PT_Semantics_ACC_Python")] %>% 
                                 scale(center = F) %>% 
                                 rowMeans(na.rm = T),
                               "syntax"=uclimb.model.dat[,c( "GR_Total_OLCTS","PT_Syntax_ACC_Python")] %>% 
                                 scale(center = F) %>%
                                 rowMeans(na.rm = T)
                               
                                 )

combined_measures %>% 
  reshape2::melt() %>% 
  summarySE(groupvars = c("model","variable" ), measurevar = "value") %>%
   ggplot(aes(model, value, group=model,fill=model))+
  geom_bar(stat = "identity", position = "dodge") +
   geom_errorbar(aes(ymin=value-se, ymax=value+se), width=.25, size=.75)+
 theme_pubr(base_size = 16)+
  facet_wrap(vars(variable))
  
```

These plots show that in the biased model, most of the subjects are at very low percentage of RL use. But also, higher rates of RL use or, more even split between RL and LTM indicates a separation between s3 and s6 learning accuracy. 
```{r parameters:biased model}
melted.p.behav.model %>%  
  filter(model=='biased') %>% 
  filter(iteration==12)%>% 
  filter(type== 'behav') %>% 
  summarySE(groupvars =c( "bias","type", "setSize") , measurevar = "accuracy") %>%
  ggplot(aes(as.factor(bias), accuracy, group=setSize,color=setSize)) + 
  geom_point()+geom_errorbar(aes(ymin=accuracy-se, ymax=accuracy+se)) +
  ylim(0.3,1)

melted.p.behav.model %>%  
  filter(model=='biased') %>% 
  filter(iteration==13)%>% 
  filter(type== 'behav') %>% 
  summarySE(groupvars =c("bias","type", "setSize") , measurevar = "accuracy") %>% 
  ggplot(aes(as.factor(bias), accuracy, group=setSize,color=setSize)) + 
  geom_point() + 
  geom_errorbar(aes(ymin=accuracy-se, ymax=accuracy+se))

```

If that is the case, is the inclusion of the RL component a vital part of their learning make-up, however small it is? This plot shows what this group would have looked like if they relied only on LTM. 

```{r}

```





## Parameters

Parameter summary: what is the spread of the parameters across participants in the models?
```{r summarize extracted parameters}
p.behav.model %>% 
  select('alpha','egs','bll','imag','ans') %>% 
 scale() %>% 
  data.frame('subjects'=p.behav.model$subjects,
  'model'=p.behav.model$model) %>% 
  #x distinct() %>% 
   filter(model!="RL") %>% 
  reshape2::melt(id.vars=c('subjects', 'model')) %>% 
  ggplot(aes( y=value,x=variable, group=as.factor(variable), fill=as.factor(variable)))+
 #geom_density(alpha=.4)+
  geom_violin()+
  # geom_point()+ 
  #geom_line()+
 # geom_jitter(height = 0)+
 facet_wrap(vars(model)) +
   theme_pubr()
  

```

```{r correlate parameters with behavioral data}

```




How about some K-means clustering? 

```{r kmeans on parameter values}

param.cluster <-
  p.behav.model %>% 
  filter(model!='RL') %>% 
  select('bll','imag','ans' ) %>% 
  scale(center = F) %>% 
   kmeans(centers = 4, iter.max = 12)

p.behav.model  %>% 
  filter(model!='RL') %>% 
  select('s3.12.behav','s6.12.behav', "s3.13.model", "s6.13.model" ) %>%
  data.frame('clu'=as.factor(param.cluster$cluster)) %>% 
  reshape2::melt(id.vars='clu') %>% 
  summarySE(measurevar = 'value', groupvars = c('clu', 'variable')) %>% 
   ggplot(aes(variable,value, group=clu,fill=clu)) + 
  #geom_bar(stat = 'identity', position = 'dodge') + 
  geom_errorbar(aes(ymin=value-se, ymax=value+se)) 

```



Some specific plans are to estimate the three LTM parameters for all 83 participants and see if they are related to WM, PSS measures. Also, how are the parameters related to the "separation" between s3 and s6? 


Some more specific things to test might be effect of delay between stimulus presentations. 






## Individual plots:

```{r show individual plots RL, fig.width=6, fig.height=6}
plot.indiv('RL', 'RL Only Model', 2)

```

```{r, fig.height=36, fig.width=18}
plot.indiv('biased', 'Integrated Model - Biased', 1)

```

```{r indiv plots, metaRL, fig.height=15, fig.width=24 }
plot.indiv('meta_RL', 'Integrated Model - Meta RL', 4)
```

```{r indiv plots LTM, fig.width=24, fig.height=72}
plot.indiv('LTM', 'LTM Only Model', 4)

```




